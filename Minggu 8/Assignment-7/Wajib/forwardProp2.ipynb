{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def forwardPass(inputs,weight,bias,activation = 'linear'):\n",
    "    w_sum = np.dot(inputs,weight) + bias\n",
    "\n",
    "    if activation is 'relu' :\n",
    "        act = np.maximum(w_sum,0)\n",
    "    else :\n",
    "        act = w_sum\n",
    "    \n",
    "    return act\n",
    "\n",
    "W_H = np.array([[0.000192761, -0.78845304, 0.30310717, 0.44131625, 0.32792646, -0.02451803, 1.43445349, -1.12972116]])\n",
    "b_H = np.array([[-0.02657719, -1.15885878, -0.3350513, -0.23438406, -0.25078532, 0.22305705, 0.80253315 ]])\n",
    "\n",
    "W_o = np.array([[-0.77540326], [0.5030424], [0.37374797], [-0.20287184], [-0.35956827], [-0.54576212], [1.04326093], [0.8857621]])\n",
    "b_o = np.array([0.04351173])\n",
    "\n",
    "inputs = np.array ([[-2], [0], [2]])\n",
    "\n",
    "h_out = forwardPass(inputs, W_H, b_H, 'relu')\n",
    "\n",
    "print ('Hidden Layer Output (ReLU)')\n",
    "print('==============================')\n",
    "print(h_out, \"\\n\")\n",
    "\n",
    "o_out = forwardPass(h_out, W_o. b_o, 'linear')\n",
    "\n",
    "print('Output Layer Output (Linear)')\n",
    "print('==============================')\n",
    "print(o_out, \"\\n\")\n",
    "\n",
    "\"\"\"[[ 2.96598907]\n",
    "[0.98707188]\n",
    "[3.00669343]\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
